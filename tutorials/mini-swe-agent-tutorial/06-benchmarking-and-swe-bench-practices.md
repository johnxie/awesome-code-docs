---
layout: default
title: "Chapter 6: Benchmarking and SWE-bench Practices"
nav_order: 6
parent: Mini-SWE-Agent Tutorial
---

# Chapter 6: Benchmarking and SWE-bench Practices

This chapter focuses on benchmark discipline and experiment quality.

## Learning Goals

- run consistent swebench evaluations
- compare model variants fairly
- capture trajectory evidence for analysis
- prevent false conclusions from uncontrolled settings

## Evaluation Checklist

- pin dataset slice and model version per run
- log config and environment metadata
- review trajectory artifacts for failure modes
- run repeat trials before ranking changes

## Source References

- [SWE-bench Usage Guide](https://mini-swe-agent.com/latest/usage/swebench/)
- [SWE-bench Website](https://www.swebench.com/)
- [Mini-SWE-Agent README](https://github.com/SWE-agent/mini-swe-agent/blob/main/README.md)

## Summary

You now have a benchmark workflow that is both rigorous and reproducible.

Next: [Chapter 7: Cookbook Extensions and Python Bindings](07-cookbook-extensions-and-python-bindings.md)
