---
layout: default
title: "Chapter 3: AI Integration"
parent: "Perplexica Tutorial"
nav_order: 3
---

# Chapter 3: AI Integration

Perplexica's intelligence comes from its seamless integration with large language models. Learn how to configure and optimize AI providers for the best search results.

## Supported AI Providers

### OpenAI GPT Models

- GPT-4 for complex analysis
- GPT-3.5-turbo for faster responses
- Fine-tuned models for specific domains

### Anthropic Claude

- Claude 3 Opus for research tasks
- Claude 3 Sonnet for balanced performance
- Claude 3 Haiku for quick responses

### Local Models

- Ollama integration for privacy
- Local LLM deployment options
- Custom model fine-tuning

## AI Configuration

### Model Selection

Configure models based on use case:

```python
# Research queries
MODEL = "gpt-4"

# Quick answers  
MODEL = "claude-3-haiku"

# Complex analysis
MODEL = "claude-3-opus"
```

### Prompt Engineering

Craft effective prompts for search:

- Query understanding prompts
- Result synthesis prompts
- Follow-up question generation
- Credibility assessment prompts

## Optimization Techniques

### Response Quality

- Temperature and top-p settings
- Max tokens configuration
- System message optimization
- Few-shot examples for consistency

### Performance Tuning

- Model caching strategies
- Request batching
- Rate limit management
- Cost optimization

## Next Steps

AI integration complete. Next, web scraping.

**Ready for web scraping? Continue to [Chapter 4: Web Scraping and Data Collection](04-web-scraping.md)**

---

*Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)*
